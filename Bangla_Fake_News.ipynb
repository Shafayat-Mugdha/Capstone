{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoiding warning\n",
    "import warnings\n",
    "def warn(*args, **kwargs): pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librarires\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "stopwords = [\"অবশ্য\",\"অনেক\",\"অনেকে\",\"অনেকেই\",\"অন্তত\",\"অথবা\",\"অথচ\",\"অর্থাত\",\"অন্য\",\"আজ\",\"আছে\",\"আপনার\",\"আপনি\",\"আবার\",\"আমরা\",\"আমাকে\",\"আমাদের\"\n",
    "             ,\"আমার\",\"আমি\",\"আরও\",\"আর\",\"আগে\",\"আগেই\",\"আই\",\"অতএব\",\"আগামী\",\"অবধি\",\"অনুযায়ী\",\"আদ্যভাগে\",\"এই\",\"একই\",\"একে\",\"একটি\",\"এখন\",\"এখনও\"\n",
    "             ,\"এখানে\",\"এখানেই\",\"এটি\",\"এটা\",\"এটাই\",\"এতটাই\",\"এবং\",\"একবার\",\"এবার\",\"এদের\",\"এঁদের\",\"এমন\",\"এমনকী\",\"এল\",\"এর\",\"এরা\",\"এঁরা\",\"এস\",\"এত\"\n",
    "             ,\"এতে\",\"এসে\",\"একে\",\"এ\",\"ঐ\",\" ই\",\"ইহা\",\"ইত্যাদি\",\"উনি\",\"উপর\",\"উপরে\",\"উচিত\",\"ও\",\"ওই\",\"ওর\",\"ওরা\",\"ওঁর\",\"ওঁরা\",\"ওকে\",\"ওদের\",\"ওঁদের\",\n",
    "             \"ওখানে\",\"কত\",\"কবে\",\"করতে\",\"কয়েক\",\"কয়েকটি\",\"করবে\",\"করলেন\",\"করার\",\"কারও\",\"করা\",\"করি\",\"করিয়ে\",\"করার\",\"করাই\",\"করলে\",\"করলেন\",\n",
    "             \"করিতে\",\"করিয়া\",\"করেছিলেন\",\"করছে\",\"করছেন\",\"করেছেন\",\"করেছে\",\"করেন\",\"করবেন\",\"করায়\",\"করে\",\"করেই\",\"কাছ\",\"কাছে\",\"কাজে\",\"কারণ\",\"কিছু\",\n",
    "             \"কিছুই\",\"কিন্তু\",\"কিংবা\",\"কি\",\"কী\",\"কেউ\",\"কেউই\",\"কাউকে\",\"কেন\",\"কে\",\"কোনও\",\"কোনো\",\"কোন\",\"কখনও\",\"ক্ষেত্রে\",\"খুব\tগুলি\",\"গিয়ে\",\"গিয়েছে\",\n",
    "             \"গেছে\",\"গেল\",\"গেলে\",\"গোটা\",\"চলে\",\"ছাড়া\",\"ছাড়াও\",\"ছিলেন\",\"ছিল\",'ছিলো',\"জন্য\",\"জানা\",\"ঠিক\",\"তিনি\",\"তিনঐ\",\"তিনিও\",\"তখন\",\"তবে\",\"তবু\",\"তাঁদের\",\n",
    "             \"তাঁাহারা\",\"তাঁরা\",\"তাঁর\",\"তাঁকে\",\"তাই\",\"তেমন\",\"তাকে\",\"তাহা\",\"তাহাতে\",\"তাহার\",\"তাদের\",\"তারপর\",\"তারা\",\"তারৈ\",\"তার\",\"তাহলে\",\"তিনি\",\"তা\",\n",
    "             \"তাও\",\"তাতে\",\"তো\",\"তত\",\"তুমি\",\"তোমার\",\"তথা\",\"থাকে\",\"থাকা\",\"থাকায়\",\"থেকে\",\"থেকেও\",\"থাকবে\",\"থাকেন\",\"থাকবেন\",\"থেকেই\",\"দিকে\",\"দিতে\",\n",
    "             \"দিয়ে\",\"দিয়েছে\",\"দিয়েছেন\",\"দিলেন\",\"দু\",\"দুটি\",\"দুটো\",\"দেয়\",\"দেওয়া\",\"দেওয়ার\",\"দেখা\",\"দেখে\",\"দেখতে\",\"দ্বারা\",\"ধরে\",\"ধরা\",\"নয়\",\"নানা\",\"না\",\n",
    "             \"নাকি\",\"নাগাদ\",\"নিতে\",\"নিজে\",\"নিজেই\",\"নিজের\",\"নিজেদের\",\"নিয়ে\",\"নেওয়া\",\"নেওয়ার\",\"নেই\",\"নাই\",\"পক্ষে\",\"পর্যন্ত\",\"পাওয়া\",\"পারেন\",\"পারি\",\"পারে\",\n",
    "             \"পরে\",\"পরেই\",\"পরেও\",\"পর\",\"পেয়ে\",\"প্রতি\",\"প্রভৃতি\",\"প্রায়\",\"ফের\",\"ফলে\",\"ফিরে\",\"ব্যবহার\",\"বলতে\",\"বললেন\",\"বলেছেন\",\"বলল\",\"বলা\",\"বলেন\",\"বলে\",\n",
    "             \"বহু\",\"বসে\",\"বার\",\"বা\",\"বিনা\",\"বরং\",\"বদলে\",\"বাদে\",\"বার\",\"বিশেষ\",\"বিভিন্ন\tবিষয়টি\",\"ব্যবহার\",\"ব্যাপারে\",\"ভাবে\",\"ভাবেই\",\"মধ্যে\",\"মধ্যেই\",\"মধ্যেও\",\n",
    "             \"মধ্যভাগে\",\"মাধ্যমে\",\"মাত্র\",\"মতো\",\"মতোই\",\"মোটেই\",\"যখন\",\"যদি\",\"যদিও\",\"যাবে\",\"যায়\",\"যাকে\",\"যাওয়া\",\"যাওয়ার\",\"যত\",\"যতটা\",\"যা\",\"যার\",\"যারা\",\n",
    "             \"যাঁর\",\"যাঁরা\",\"যাদের\",\"যান\",\"যাচ্ছে\",\"যেতে\",\"যাতে\",\"যেন\",\"যেমন\",\"যেখানে\",\"যিনি\",\"যে\",\"রেখে\",\"রাখা\",\"রয়েছে\",\"রকম\",\"শুধু\",\"সঙ্গে\",\"সঙ্গেও\",\n",
    "             \"সমস্ত\",\"সব\",\"সবার\",\"সহ\",\"সুতরাং\",\"সহিত\",\"সেই\",\"সেটা\",\"সেটি\",\"সেটাই\",\"সেটাও\",\"সম্প্রতি\",\"সেখান\",\"সেখানে\",\"সে\",\"স্পষ্ট\",\"স্বয়ং\",\"হইতে\",\"হইবে\",\n",
    "             \"হৈলে\",\"হইয়া\",\"হচ্ছে\",\"হত\",\"হতে\",\"হতেই\",\"হবে\",\"হবেন\",\"হয়েছিল\",\"হয়েছে\",\"হয়েছেন\",\"হয়ে\",\"হয়নি\",\"হয়\",\"হয়েই\",\"হয়তো\",\"হল\",\"হলে\",\"হলেই\",\"হলেও\",\n",
    "             \"হলো\",\"হিসাবে\",\"হওয়া\",\"হওয়ার\",\"হওয়ায়\",\"হন\",\"হোক\",\"জন\",\"জনকে\",\"জনের\",\"জানতে\",\"জানায়\",\"জানিয়ে\",\"জানানো\",\"জানিয়েছে\",\"জন্য\",\"জন্যওজে\",\n",
    "             \"জে\",\"বেশ\",\"দেন\",\"তুলে\",\"ছিলেন\",\"চান\",\"চায়\",\"চেয়ে\",\"মোট\",\"যথেষ্ট\",\"টি\"]\n",
    "# stopwords = stopwords.words('english')\n",
    "# stopwords.remove('not')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from  sklearn.metrics  import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removing special characters from a sentence/review/data\n",
    "special_characters = string.punctuation + ',?!।‘’'\n",
    "numeric_characters = '0123456789০১২৩৪৫৬৭৮৯'\n",
    "def remove_special_characters(sentence):\n",
    "    updated_sentence = \"\".join([char for char in sentence if char not in special_characters])\n",
    "    updated_sentence = \"\".join([char for char in updated_sentence if char not in numeric_characters])\n",
    "    return updated_sentence\n",
    "\n",
    "# Function for tokenizing the sentence \n",
    "def tokenize(sentence):\n",
    "    tokens = sentence.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "# Function for removing stopwords\n",
    "def remove_stopwords(tokenized_list):\n",
    "    updated_tokenized_list = [word for word in tokenized_list if word not in stopwords]\n",
    "    new_word_list = []\n",
    "    newline_status = 0\n",
    "    special_character_status = 0\n",
    "    for word in updated_tokenized_list:\n",
    "        if \"\\n\" in word:\n",
    "            word = re.sub('\\n', ' ', word)\n",
    "            if \"\\xa0\" in word:\n",
    "                word = re.sub('\\xa0', ' ', word)\n",
    "            if ' ' in word:\n",
    "                collection = word.split(\" \")\n",
    "                for c in collection:\n",
    "                    newline_status = 1\n",
    "                    new_word_list.append(c)\n",
    "                 \n",
    "        if \"\\xa0\" in word:\n",
    "            word = re.sub('\\xa0', ' ', word)\n",
    "            if \"\\n\" in word:\n",
    "                word = re.sub('\\xa0', ' ', word)\n",
    "            if ' ' in word:\n",
    "                collection = word.split(\" \")\n",
    "                for c in collection:\n",
    "                    special_character_status = 1\n",
    "                    new_word_list.append(c)\n",
    "                \n",
    "        if newline_status == 1:\n",
    "            pass\n",
    "        elif special_character_status == 1:\n",
    "            pass\n",
    "        else:\n",
    "            new_word_list.append(word)    \n",
    "    return list(set(new_word_list))\n",
    "\n",
    "# Function for lemmatizing (Keeps the sentence's context)\n",
    "def lemmatizing(tokenized_list):\n",
    "    lemmatizing_list = Noun_Stemmer(' '.join(tokenized_list)) ###### change noun or verbal\n",
    "    return lemmatizing_list\n",
    "\n",
    "# Function for merging words to create a sentence/review/data\n",
    "def words_merging(lemmatized_list):\n",
    "    sentence = ' '.join(str(x) for x in lemmatized_list)\n",
    "    return sentence\n",
    "\n",
    "#Function for Noun Stemming\n",
    "def Noun_Stemmer_Step_1(sentence):\n",
    "    word_list     = sentence.split(\" \")\n",
    "    new_word_list = []\n",
    "    independant_inflections_1 = 'তে,কে,রা,দে,কা,রা'.split(\",\")\n",
    "    independant_inflections_2 = 'গুলি,গুলো,দের,গুলোতে'.split(\",\")\n",
    "    single_inflections_character = 'া,ো,ে,ি,ী'.split(\",\")\n",
    "    for word in word_list:\n",
    "        flag = 0\n",
    "        for i_inflections_1 in independant_inflections_1:\n",
    "            if i_inflections_1 in word:\n",
    "                word = re.sub(i_inflections_1+'$', '', word)\n",
    "                flag = 1\n",
    "                break\n",
    "        \n",
    "        status = 0\n",
    "        characters = list(word)\n",
    "        clean_list = [x for x in characters if x not in single_inflections_character]\n",
    "        length = len(clean_list)\n",
    "        if length > 3:\n",
    "            status = 1\n",
    "        \n",
    "        if status == 1:\n",
    "            for i_inflections_2 in independant_inflections_2:\n",
    "                if i_inflections_2 in word:\n",
    "                    word = re.sub(i_inflections_2+'$', '', word)\n",
    "                    break\n",
    "                \n",
    "        new_word_list.append(word)\n",
    "\n",
    "    return ' '.join(new_word_list)      \n",
    "    \n",
    "def Noun_Stemmer_Step_2(sentence):\n",
    "    word_list     = sentence.split(\" \")\n",
    "    new_word_list = []\n",
    "    independant_inflections = ['য়ের']\n",
    "    single_inflections_character = 'া,ো,ে,ি,ী'.split(\",\")\n",
    "    vowels = 'অ,আ,ই,ঈ,উ,ঊ,ঋ,এ,ঐ,ও,ঔ'.split(\",\")\n",
    "    for word in word_list:\n",
    "        for i_inflections in independant_inflections:\n",
    "                if i_inflections in word:\n",
    "                    temp = re.sub(i_inflections+'$', '', word)\n",
    "                    characters = list(temp)\n",
    "                    clean_list = [x for x in characters if x not in single_inflections_character]\n",
    "                    length = len(clean_list)\n",
    "                    if length < 2:\n",
    "                        word = re.sub(i_inflections+'$', '', word)\n",
    "                        break\n",
    "                    elif clean_list[-1] in vowels:\n",
    "                        word = re.sub(i_inflections+'$', '', word)\n",
    "                        break\n",
    "                    else:\n",
    "                        word = word[:-2]\n",
    "                        break\n",
    "        for single_inflection in single_inflections_character:\n",
    "            if  len(word) > 1 and single_inflection in word[-2] and word[-1] == 'র':\n",
    "                word = word[:-1]\n",
    "                if word[-1] == 'ে':\n",
    "                    word = word[:-1]\n",
    "                    break\n",
    "                \n",
    "        new_word_list.append(word)\n",
    "    \n",
    "    return ' '.join(new_word_list) \n",
    "\n",
    "def Noun_Stemmer_Step_3(sentence):\n",
    "    word_list     = sentence.split(\" \")\n",
    "    new_word_list = []\n",
    "    for word in word_list:\n",
    "        if word != \"\":\n",
    "            if word[-1] == 'র':\n",
    "                index = word.find('ষ')\n",
    "                if index != -1:\n",
    "                    if index < len(word) - 2: \n",
    "                        if word[index+2]  == 'ট':\n",
    "                            word = re.sub('র'+'$', '', word)\n",
    "                        elif 'টির' in word:\n",
    "                            word = re.sub('টির'+'$', '', word)\n",
    "            elif 'টি' in word:\n",
    "                word = re.sub('টি'+'$', '', word)\n",
    "            elif 'টা' in word:\n",
    "                word = re.sub('টা'+'$', '', word)\n",
    "            elif 'টির' in word:\n",
    "                word = re.sub('টির'+'$', '', word)\n",
    "            elif 'জন' in word:\n",
    "                word = re.sub('জন'+'$', '', word)\n",
    "            elif 'খানা' in word:\n",
    "                word = re.sub('খানা'+'$', '', word)\n",
    "            new_word_list.append(word)\n",
    "    \n",
    "    return ' '.join(new_word_list)   \n",
    "\n",
    "def Noun_Stemmer(sentence):\n",
    "    stemmed_sentence = Noun_Stemmer_Step_1(sentence)\n",
    "    stemmed_sentence = Noun_Stemmer_Step_2(stemmed_sentence)\n",
    "    stemmed_sentence = Noun_Stemmer_Step_3(stemmed_sentence)\n",
    "    return stemmed_sentence.split(\" \")\n",
    "\n",
    "# Function for Bangla Stemming\n",
    "def Verbal_Stemmer_Step_1(sentence):\n",
    "    word_list = sentence.split(\" \")\n",
    "    new_word_list = []\n",
    "    independant_inflections_1 = 'ই,ছ,ত,ব,ল,ন,ক,স,ম'.split(\",\")\n",
    "    independant_inflections_2 = 'লা,লো,তো,লে,লে,তা,তি,ছি,ছে,ছো,তে,লি,বে'.split(\",\")\n",
    "    combined_inflections = 'ছিলাম,ছিলেন,ছেন,লাম,লেন,তেন,তাম,বেন'.split(\",\")\n",
    "    single_inflections_character = 'া,ো,ে,ি,ী'.split(\",\")\n",
    "    for word in word_list:\n",
    "        if word == 'আন':\n",
    "            word = 'আনা'\n",
    "            new_word_list.append(word)\n",
    "        elif word == 'আস':\n",
    "            word = 'আসা'\n",
    "            new_word_list.append(word)\n",
    "        elif word == 'আস':\n",
    "            word = 'আসা'\n",
    "            new_word_list.append(word)\n",
    "        elif word == 'আয়':\n",
    "            word = 'আসা'\n",
    "            new_word_list.append(word)\n",
    "        elif word == 'এলেন':\n",
    "            word = 'আসা'\n",
    "            new_word_list.append(word)\n",
    "        elif word == 'এসেছিলাম':\n",
    "            word = 'আসা'\n",
    "            new_word_list.append(word)\n",
    "        elif word == 'গিয়েছিলাম':\n",
    "            word = 'যাওয়া'\n",
    "            new_word_list.append(word)\n",
    "        elif word == 'খাচ্ছিলাম':\n",
    "            word = 'খাওয়া'\n",
    "            new_word_list.append(word)\n",
    "        else:\n",
    "            status = 0\n",
    "            characters = list(word)\n",
    "            length1 = len(characters)\n",
    "            clean_list = [x for x in characters if x not in single_inflections_character]\n",
    "            length2 = len(clean_list)\n",
    "            if length1 == length2 and length1 < 3:\n",
    "                status = 1\n",
    "\n",
    "            if status == 1:\n",
    "                new_word_list.append(word)\n",
    "            elif word == 'দে':\n",
    "                new_word_list.append(word)\n",
    "            elif word == 'খা':\n",
    "                new_word_list.append(word)\n",
    "            elif  word[-1] == 'া':\n",
    "                new_word_list.append(word)\n",
    "            else:\n",
    "                status = 0\n",
    "                for c_inflections in combined_inflections:\n",
    "                    if c_inflections in word:\n",
    "                        status = 1\n",
    "                        word = re.sub(c_inflections+'$', '', word)\n",
    "                if status == 0:\n",
    "                    for i_inflections_1 in independant_inflections_1:\n",
    "                        if len(list(word)) > 2:\n",
    "                            if word[-1] == i_inflections_1:\n",
    "                                word = word[:-1]\n",
    "                                if word[-1] in single_inflections_character:\n",
    "                                    for i_inflections_2 in independant_inflections_2:\n",
    "                                        word_part1 = word[-1]\n",
    "                                        word_part2 = word[-2]\n",
    "                                        inflection_list = list(i_inflections_2)\n",
    "                                        inflection_part1 = inflection_list[1]\n",
    "                                        inflection_part2 = inflection_list[0]\n",
    "                                        if (word_part1 == inflection_part1) and (word_part2 == inflection_part2):\n",
    "                                            word = word[:-2]\n",
    "                                            break\n",
    "                            else:\n",
    "                                if len(list(word)) == 3:\n",
    "                                    if word[-1] == 'ে':\n",
    "                                        word = word[:-1]\n",
    "                                elif word[-1] in single_inflections_character:\n",
    "                                    for i_inflections_2 in independant_inflections_2:\n",
    "                                        word_part1 = word[-1]\n",
    "                                        word_part2 = word[-2]\n",
    "                                        inflection_list = list(i_inflections_2)\n",
    "                                        inflection_part1 = inflection_list[1]\n",
    "                                        inflection_part2 = inflection_list[0]\n",
    "                                        if (word_part1 == inflection_part1) and (word_part2 == inflection_part2):\n",
    "                                            word = word[:-2]\n",
    "                                            break\n",
    "                new_word_list.append(word)\n",
    "        \n",
    "    return ' '.join(new_word_list)\n",
    "    \n",
    "def Verbal_Stemmer_Step_2(sentence):\n",
    "    word_list = sentence.split(\" \")\n",
    "    new_word_list = []\n",
    "    single_inflections_character = 'া,ো,ে,ি,ী'.split(\",\")\n",
    "    for word in word_list:\n",
    "        status = 0\n",
    "        characters = list(word)\n",
    "        length1 = len(characters)\n",
    "        clean_list = [x for x in characters if x not in single_inflections_character]\n",
    "        length2 = len(clean_list)\n",
    "        if length1 == length2 and length1 < 3:\n",
    "            status = 1\n",
    "        \n",
    "        if status == 1:\n",
    "            new_word_list.append(word)\n",
    "        elif word == 'দে':\n",
    "            new_word_list.append(word)\n",
    "        elif word == 'খা':\n",
    "            new_word_list.append(word)\n",
    "        elif  word[-1] == 'া':\n",
    "            new_word_list.append(word)\n",
    "        else:        \n",
    "            length = 0\n",
    "            characters = list(word)\n",
    "            clean_list = [x for x in characters if x not in single_inflections_character]\n",
    "            length = len(clean_list)\n",
    "            if length < 3:\n",
    "                if word[-1] == 'য়' or word[-1] == 'ও':\n",
    "                    word = word[:-1]\n",
    "                    word += 'ওয়া'\n",
    "                    new_word_list.append(word)\n",
    "                elif word == 'বল':\n",
    "                    word = 'বলা'\n",
    "                    new_word_list.append(word)\n",
    "                else:\n",
    "                    for s_i_inflections in single_inflections_character:\n",
    "                        if word[-1] == s_i_inflections:\n",
    "                            word = word[:-1]\n",
    "                            if word[-1] == 'য়' or word[-1] == 'ও':\n",
    "                                word = word[:-1]\n",
    "                                word += 'ওয়া'\n",
    "                                new_word_list.append(word)\n",
    "    return ' '.join(new_word_list)\n",
    "\n",
    "def Verbal_Stemmer_Step_3(sentence):\n",
    "    word_list = sentence.split(\" \")\n",
    "    new_word_list = []\n",
    "    single_inflections_character = 'া,ো,ে,ি,ী'.split(\",\")\n",
    "    for word in word_list:\n",
    "        words = list(word)\n",
    "        if len(words) > 2 and words[1] in single_inflections_character:\n",
    "            if words[1] == 'ু':\n",
    "                words[1] = 'ো'\n",
    "            elif words[1] == 'ি':\n",
    "                words[1] = 'ে'\n",
    "            elif words[1] == 'ে':\n",
    "                words[1] = 'া'\n",
    "        new_word_list.append(''.join(words))        \n",
    "\n",
    "    \n",
    "    return ' '.join(new_word_list)\n",
    "    \n",
    "def Verbal_Stemmer(sentence):\n",
    "    stemmed_sentence = Verbal_Stemmer_Step_1(sentence)\n",
    "    stemmed_sentence = Verbal_Stemmer_Step_2(stemmed_sentence)\n",
    "    stemmed_sentence = Verbal_Stemmer_Step_3(stemmed_sentence)\n",
    "    return stemmed_sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>চলে গেলেন মীনা কার্টুনের ‘সেই’ রাম মোহন</td>\n",
       "      <td>চলে গেলেন দক্ষিণ এশিয়ার সবচেয়ে জনপ্রিয় কার্টুন...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>সম্রাটকে গ্রেপ্তারে যুবলীগে কেউ ক্ষুব্ধ  কেউ উ...</td>\n",
       "      <td>দাপুটে নেতা ইসমাইল চৌধুরী সম্রাট গ্রেপ্তার হওয়...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>রানু মণ্ডল মারা গেছেন!</td>\n",
       "      <td>লতা মঙ্গেশকরের ‘প্যায়ার কা নগমা’ গান গেয়ে ভাইর...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>শিবির সন্দেহে বুয়েট শিক্ষার্থী আবরার ফাহাদকে প...</td>\n",
       "      <td>শিবির সন্দেহে বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ের...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি!</td>\n",
       "      <td>যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি! সাম্প্রতি ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>টেস্ট ক্রিকেটের প্রশিক্ষণের জন্য ক্রিকেট দলকে ...</td>\n",
       "      <td>উগান্ডার বেশির ভাগ মানুষ নিরাপদ পানি থেকে বঞ্চ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>আবরারকে যেভাবে হত্যা করা হয়</td>\n",
       "      <td>বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ের (বুয়েট) শেরেব...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‘কন্ডাক্টর ইচ্ছে করেই বাস চালিয়ে দিয়ে আবরারকে ...</td>\n",
       "      <td>রাজধানীর নর্দ্দায় সুপ্রভাত পরিবহনের বাসের চাপা...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>পিএসজি ছেড়ে কলাবাগান  মোহামেডান বা ভিক্টোরিয়া ...</td>\n",
       "      <td>সম্প্রতি ঢাকার বিভিন্ন ক্লাবে অভিযান চালিয়ে অস...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>আবরারের ছোট ভাইকে পুলিশের মারধর</td>\n",
       "      <td>বুয়েট ছাত্র আবরার ফাহাদের ছোট ভাই ফায়াজকে মারধ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>কেন উইলিয়ামসনকে অবসর নেয়ার পরামর্শ দিলেন লিওনে...</td>\n",
       "      <td>লর্ডসে অনুষ্ঠিত অনবদ্য বিশ্বকাপ ফাইনালে দুইবার...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>আগামী অলিম্পিকে ভারোত্তোলনে অংশ নিতে যাচ্ছেন স...</td>\n",
       "      <td>২০১৯ ক্রিকেট বিশ্বকাপে বাংলাদেশ দল প্রত্যাশা প...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>মোদি সৈকতে প্লাস্টিকের বোতল কুড়াচ্ছেন</td>\n",
       "      <td>হাতভর্তি প্লাস্টিকের পরিত্যক্ত বোতল। সৈকতে খাল...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>টেলিভিশন নাটকে অনাপত্তিপত্র নিয়ে মিশ্র প্রতিক্...</td>\n",
       "      <td>টেলিভিশন নাটক নিয়ে অভিযোগ পুরোনো। মানহীন নাটক ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>এ টি এম শামসুজ্জামানকে আবার লাইফ সাপোর্ট</td>\n",
       "      <td>কয়েক দিন ধরে দেশবরেণ্য অভিনয়শিল্পী এ টি এম শাম...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ট্রফি নিয়েই ফিরছে বাংলাদেশের মেয়েরা</td>\n",
       "      <td>সেমিফাইনালে আয়ারল্যান্ডকে হারিয়ে বাছাইপর্বের ফ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>এ হারেও ইতিবাচক কিছু পাচ্ছেন সাকিব</td>\n",
       "      <td>আফগানিস্তানের বিপক্ষে টেস্টে হারার পরই সাকিব আ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>শেখ হাসিনাকে ‘মা’ ডাকলেন রানি মুখার্জি</td>\n",
       "      <td>বাংলাদেশের প্রধানমন্ত্রী শেখ হাসিনার ব্যক্তিত্...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>রেনিটিডিনে ক্যান্সারের উপাদান  দেশে দেশে সতর্কতা</td>\n",
       "      <td>অ্যাসিড নিঃসরণ প্রতিরোধসহ পেটের পীড়ার নানা উপস...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ডেঙ্গুতে মরছে  কিন্তু সরকারি তালিকায় আসছে না</td>\n",
       "      <td>ঢাকায় ব্যাপক হারে ছড়িয়ে পড়া ডেঙ্গু রোগে অনেকের...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0             চলে গেলেন মীনা কার্টুনের ‘সেই’ রাম মোহন   \n",
       "1   সম্রাটকে গ্রেপ্তারে যুবলীগে কেউ ক্ষুব্ধ  কেউ উ...   \n",
       "2                              রানু মণ্ডল মারা গেছেন!   \n",
       "3   শিবির সন্দেহে বুয়েট শিক্ষার্থী আবরার ফাহাদকে প...   \n",
       "4                 যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি!   \n",
       "5   টেস্ট ক্রিকেটের প্রশিক্ষণের জন্য ক্রিকেট দলকে ...   \n",
       "6                         আবরারকে যেভাবে হত্যা করা হয়   \n",
       "7   ‘কন্ডাক্টর ইচ্ছে করেই বাস চালিয়ে দিয়ে আবরারকে ...   \n",
       "8   পিএসজি ছেড়ে কলাবাগান  মোহামেডান বা ভিক্টোরিয়া ...   \n",
       "9                     আবরারের ছোট ভাইকে পুলিশের মারধর   \n",
       "10  কেন উইলিয়ামসনকে অবসর নেয়ার পরামর্শ দিলেন লিওনে...   \n",
       "11  আগামী অলিম্পিকে ভারোত্তোলনে অংশ নিতে যাচ্ছেন স...   \n",
       "12              মোদি সৈকতে প্লাস্টিকের বোতল কুড়াচ্ছেন   \n",
       "13  টেলিভিশন নাটকে অনাপত্তিপত্র নিয়ে মিশ্র প্রতিক্...   \n",
       "14           এ টি এম শামসুজ্জামানকে আবার লাইফ সাপোর্ট   \n",
       "15                ট্রফি নিয়েই ফিরছে বাংলাদেশের মেয়েরা   \n",
       "16                 এ হারেও ইতিবাচক কিছু পাচ্ছেন সাকিব   \n",
       "17             শেখ হাসিনাকে ‘মা’ ডাকলেন রানি মুখার্জি   \n",
       "18   রেনিটিডিনে ক্যান্সারের উপাদান  দেশে দেশে সতর্কতা   \n",
       "19       ডেঙ্গুতে মরছে  কিন্তু সরকারি তালিকায় আসছে না   \n",
       "\n",
       "                                                 Body Label  \n",
       "0   চলে গেলেন দক্ষিণ এশিয়ার সবচেয়ে জনপ্রিয় কার্টুন...     1  \n",
       "1   দাপুটে নেতা ইসমাইল চৌধুরী সম্রাট গ্রেপ্তার হওয়...     1  \n",
       "2   লতা মঙ্গেশকরের ‘প্যায়ার কা নগমা’ গান গেয়ে ভাইর...     0  \n",
       "3   শিবির সন্দেহে বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ের...     0  \n",
       "4   যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি! সাম্প্রতি ...     0  \n",
       "5   উগান্ডার বেশির ভাগ মানুষ নিরাপদ পানি থেকে বঞ্চ...     0  \n",
       "6   বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ের (বুয়েট) শেরেব...     1  \n",
       "7   রাজধানীর নর্দ্দায় সুপ্রভাত পরিবহনের বাসের চাপা...     1  \n",
       "8   সম্প্রতি ঢাকার বিভিন্ন ক্লাবে অভিযান চালিয়ে অস...     0  \n",
       "9   বুয়েট ছাত্র আবরার ফাহাদের ছোট ভাই ফায়াজকে মারধ...     1  \n",
       "10  লর্ডসে অনুষ্ঠিত অনবদ্য বিশ্বকাপ ফাইনালে দুইবার...     0  \n",
       "11  ২০১৯ ক্রিকেট বিশ্বকাপে বাংলাদেশ দল প্রত্যাশা প...     0  \n",
       "12  হাতভর্তি প্লাস্টিকের পরিত্যক্ত বোতল। সৈকতে খাল...     1  \n",
       "13  টেলিভিশন নাটক নিয়ে অভিযোগ পুরোনো। মানহীন নাটক ...     1  \n",
       "14  কয়েক দিন ধরে দেশবরেণ্য অভিনয়শিল্পী এ টি এম শাম...     1  \n",
       "15  সেমিফাইনালে আয়ারল্যান্ডকে হারিয়ে বাছাইপর্বের ফ...     1  \n",
       "16  আফগানিস্তানের বিপক্ষে টেস্টে হারার পরই সাকিব আ...     1  \n",
       "17  বাংলাদেশের প্রধানমন্ত্রী শেখ হাসিনার ব্যক্তিত্...     1  \n",
       "18  অ্যাসিড নিঃসরণ প্রতিরোধসহ পেটের পীড়ার নানা উপস...     1  \n",
       "19  ঢাকায় ব্যাপক হারে ছড়িয়ে পড়া ডেঙ্গু রোগে অনেকের...     1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_excel(\"C:/Users/USER/capstone/mugdha.xlsx\",dtype='<U13')\n",
    "df = pd.DataFrame({'Title': dataframe.iloc[0:,0], 'Body': dataframe.iloc[0:,1], 'Label': dataframe.iloc[0:,4]}) \n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "      <th>Body_removed_special_characters</th>\n",
       "      <th>Body_tokens</th>\n",
       "      <th>Body_no_stopwords</th>\n",
       "      <th>Body_lemmatized</th>\n",
       "      <th>Body_lemmatized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>চলে গেলেন মীনা কার্টুনের ‘সেই’ রাম মোহন</td>\n",
       "      <td>চলে গেলেন দক্ষিণ এশিয়ার সবচেয়ে জনপ্রিয় কার্টুন...</td>\n",
       "      <td>1</td>\n",
       "      <td>চলে গেলেন দক্ষিণ এশিয়ার সবচেয়ে জনপ্রিয় কার্টুন...</td>\n",
       "      <td>[চলে, গেলেন, দক্ষিণ, এশিয়ার, সবচেয়ে, জনপ্রিয়, ...</td>\n",
       "      <td>[, এক্সপ্রেস, সবচেয়ে, মৃত্যুবরণ, এর, রূপদানকার...</td>\n",
       "      <td>[এক্সপ্রেস, সবচেয়ে, মৃত্যুবরণ, এর, রূপদানকারী,...</td>\n",
       "      <td>এক্সপ্রেস সবচেয়ে মৃত্যুবরণ এর রূপদানকারী তথ্য ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>সম্রাটকে গ্রেপ্তারে যুবলীগে কেউ ক্ষুব্ধ  কেউ উ...</td>\n",
       "      <td>দাপুটে নেতা ইসমাইল চৌধুরী সম্রাট গ্রেপ্তার হওয়...</td>\n",
       "      <td>1</td>\n",
       "      <td>দাপুটে নেতা ইসমাইল চৌধুরী সম্রাট গ্রেপ্তার হওয়...</td>\n",
       "      <td>[দাপুটে, নেতা, ইসমাইল, চৌধুরী, সম্রাট, গ্রেপ্ত...</td>\n",
       "      <td>[, তখন, উচ্ছ্বাস, ক্ষুব্ধদের, এর, ইসমাইল, দলের...</td>\n",
       "      <td>[তখন, উচ্ছ্বাস, ক্ষুব্ধ, এর, ইসমাইল, দল, কোনো,...</td>\n",
       "      <td>তখন উচ্ছ্বাস ক্ষুব্ধ এর ইসমাইল দল কোনো গ্রেপ্ত...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>রানু মণ্ডল মারা গেছেন!</td>\n",
       "      <td>লতা মঙ্গেশকরের ‘প্যায়ার কা নগমা’ গান গেয়ে ভাইর...</td>\n",
       "      <td>0</td>\n",
       "      <td>লতা মঙ্গেশকরের প্যায়ার কা নগমা গান গেয়ে ভাইরাল...</td>\n",
       "      <td>[লতা, মঙ্গেশকরের, প্যায়ার, কা, নগমা, গান, গেয়ে...</td>\n",
       "      <td>[, নামের, আরোগ্য, কা, বলিউডে, ভিডিওতে, ক্যাপশন...</td>\n",
       "      <td>[নাম, আরোগ্য, বলিউডে, ভিডিও, ক্যাপশন, ভাইরাল, ...</td>\n",
       "      <td>নাম আরোগ্য বলিউডে ভিডিও ক্যাপশন ভাইরাল কামনা স...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>শিবির সন্দেহে বুয়েট শিক্ষার্থী আবরার ফাহাদকে প...</td>\n",
       "      <td>শিবির সন্দেহে বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ের...</td>\n",
       "      <td>0</td>\n",
       "      <td>শিবির সন্দেহে বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ের...</td>\n",
       "      <td>[শিবির, সন্দেহে, বাংলাদেশ, প্রকৌশল, বিশ্ববিদ্য...</td>\n",
       "      <td>[, আবরার, শেরে, সারা, ব্যবস্থা, শিক্ষার্থীকে, ...</td>\n",
       "      <td>[আবরা, শেরে, সা, ব্যবস্থা, শিক্ষার্থী, জানান, ...</td>\n",
       "      <td>আবরা শেরে সা ব্যবস্থা শিক্ষার্থী জানান পুলিশ অ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি!</td>\n",
       "      <td>যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি! সাম্প্রতি ...</td>\n",
       "      <td>0</td>\n",
       "      <td>যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি সাম্প্রতি ব...</td>\n",
       "      <td>[যুবলীগের, চেয়ারম্যান, হচ্ছেন, মাশরাফি, সাম্প্...</td>\n",
       "      <td>[, পদ্ধতিতে, দেবেন, আলোচনায়, রাজনৈতিক, মুর্তজা...</td>\n",
       "      <td>[পদ্ধতি, দেবেন, আলোচনায়, রাজনৈতিক, মুর্তজা, অ,...</td>\n",
       "      <td>পদ্ধতি দেবেন আলোচনায় রাজনৈতিক মুর্তজা অ অনিবার...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0            চলে গেলেন মীনা কার্টুনের ‘সেই’ রাম মোহন   \n",
       "1  সম্রাটকে গ্রেপ্তারে যুবলীগে কেউ ক্ষুব্ধ  কেউ উ...   \n",
       "2                             রানু মণ্ডল মারা গেছেন!   \n",
       "3  শিবির সন্দেহে বুয়েট শিক্ষার্থী আবরার ফাহাদকে প...   \n",
       "4                যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি!   \n",
       "\n",
       "                                                Body Label  \\\n",
       "0  চলে গেলেন দক্ষিণ এশিয়ার সবচেয়ে জনপ্রিয় কার্টুন...     1   \n",
       "1  দাপুটে নেতা ইসমাইল চৌধুরী সম্রাট গ্রেপ্তার হওয়...     1   \n",
       "2  লতা মঙ্গেশকরের ‘প্যায়ার কা নগমা’ গান গেয়ে ভাইর...     0   \n",
       "3  শিবির সন্দেহে বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ের...     0   \n",
       "4  যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি! সাম্প্রতি ...     0   \n",
       "\n",
       "                     Body_removed_special_characters  \\\n",
       "0  চলে গেলেন দক্ষিণ এশিয়ার সবচেয়ে জনপ্রিয় কার্টুন...   \n",
       "1  দাপুটে নেতা ইসমাইল চৌধুরী সম্রাট গ্রেপ্তার হওয়...   \n",
       "2  লতা মঙ্গেশকরের প্যায়ার কা নগমা গান গেয়ে ভাইরাল...   \n",
       "3  শিবির সন্দেহে বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ের...   \n",
       "4  যুবলীগের চেয়ারম্যান হচ্ছেন মাশরাফি সাম্প্রতি ব...   \n",
       "\n",
       "                                         Body_tokens  \\\n",
       "0  [চলে, গেলেন, দক্ষিণ, এশিয়ার, সবচেয়ে, জনপ্রিয়, ...   \n",
       "1  [দাপুটে, নেতা, ইসমাইল, চৌধুরী, সম্রাট, গ্রেপ্ত...   \n",
       "2  [লতা, মঙ্গেশকরের, প্যায়ার, কা, নগমা, গান, গেয়ে...   \n",
       "3  [শিবির, সন্দেহে, বাংলাদেশ, প্রকৌশল, বিশ্ববিদ্য...   \n",
       "4  [যুবলীগের, চেয়ারম্যান, হচ্ছেন, মাশরাফি, সাম্প্...   \n",
       "\n",
       "                                   Body_no_stopwords  \\\n",
       "0  [, এক্সপ্রেস, সবচেয়ে, মৃত্যুবরণ, এর, রূপদানকার...   \n",
       "1  [, তখন, উচ্ছ্বাস, ক্ষুব্ধদের, এর, ইসমাইল, দলের...   \n",
       "2  [, নামের, আরোগ্য, কা, বলিউডে, ভিডিওতে, ক্যাপশন...   \n",
       "3  [, আবরার, শেরে, সারা, ব্যবস্থা, শিক্ষার্থীকে, ...   \n",
       "4  [, পদ্ধতিতে, দেবেন, আলোচনায়, রাজনৈতিক, মুর্তজা...   \n",
       "\n",
       "                                     Body_lemmatized  \\\n",
       "0  [এক্সপ্রেস, সবচেয়ে, মৃত্যুবরণ, এর, রূপদানকারী,...   \n",
       "1  [তখন, উচ্ছ্বাস, ক্ষুব্ধ, এর, ইসমাইল, দল, কোনো,...   \n",
       "2  [নাম, আরোগ্য, বলিউডে, ভিডিও, ক্যাপশন, ভাইরাল, ...   \n",
       "3  [আবরা, শেরে, সা, ব্যবস্থা, শিক্ষার্থী, জানান, ...   \n",
       "4  [পদ্ধতি, দেবেন, আলোচনায়, রাজনৈতিক, মুর্তজা, অ,...   \n",
       "\n",
       "                            Body_lemmatized_sentence  \n",
       "0  এক্সপ্রেস সবচেয়ে মৃত্যুবরণ এর রূপদানকারী তথ্য ...  \n",
       "1  তখন উচ্ছ্বাস ক্ষুব্ধ এর ইসমাইল দল কোনো গ্রেপ্ত...  \n",
       "2  নাম আরোগ্য বলিউডে ভিডিও ক্যাপশন ভাইরাল কামনা স...  \n",
       "3  আবরা শেরে সা ব্যবস্থা শিক্ষার্থী জানান পুলিশ অ...  \n",
       "4  পদ্ধতি দেবেন আলোচনায় রাজনৈতিক মুর্তজা অ অনিবার...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['Title_remove_special_characters']   = df['Title'].apply(lambda sentence: remove_special_characters(sentence))\n",
    "# df['Title_tokens']                      = df['Title_remove_special_characters'].apply(lambda sentence: tokenize(sentence))\n",
    "# df['Title_no_stopwords']                = df['Title_tokens'].apply(lambda sentence: remove_stopwords(sentence))\n",
    "# df['Title_lemmatized']                  = df['Title_no_stopwords'].apply(lambda sentence: lemmatizing(sentence))\n",
    "# df['Title_lemmatized_sentence']         = df['Title_lemmatized'].apply(lambda sentence: words_merging(sentence))\n",
    "\n",
    "df['Body_removed_special_characters']   = df['Body'].apply(lambda sentence: remove_special_characters(sentence))\n",
    "df['Body_tokens']                      = df['Body_removed_special_characters'].apply(lambda sentence: tokenize(sentence))\n",
    "df['Body_no_stopwords']                = df['Body_tokens'].apply(lambda sentence: remove_stopwords(sentence))\n",
    "df['Body_lemmatized']                  = df['Body_no_stopwords'].apply(lambda sentence: lemmatizing(sentence))\n",
    "df['Body_lemmatized_sentence']         = df['Body_lemmatized'].apply(lambda sentence: words_merging(sentence))\n",
    "df.head()\n",
    "\n",
    "# dataframe = pd.DataFrame({'Title_lemmatized_sentence': df.iloc[0:,12], 'Body_lemmatized_sentence': df.iloc[0:,7], 'Label': df.iloc[0:,2]})\n",
    "# # dataframe = pd.DataFrame({'Body_lemmatized_sentence': df.iloc[0:,7], 'Label': df.iloc[0:,2]})\n",
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "চলে গেলেন মীনা কার্টুনের ‘সেই’ রাম মোহনচলে গেলেন দক্ষিণ এশিয়ার সবচেয়ে জনপ্রিয় কার্টুন চরিত্র মীনার রূপদানকারী রাম মোহন। ভারতীয় অ্যানিমেশনের জনক বলে খ্যাত এ কার্টুনিস্ট ৮৮ বছর বয়সে মৃত্যুবরণ করেছেন। শুক্রবার (১১ অক্টোবর) এ তথ্য জানিয়েছে ভারতের অ্যানিমেশন বিষয়ক ওয়েবসাইট অ্যানিমেশন এক্সপ্রেস।\n",
      "\n",
      "জানা যায়  খ্যাতনামা এ কার্টুনিস্টের ভারতীয়\n",
      " চলচ্চিত্রের জগতে কর্মজীবন শুরু হয়েছিল ১৯৫৬ সালে। ১৯৬৮ সালে তিনি \n",
      "চলচ্চিত্র বিভাগ থেকে সরে দাঁড়ান  কাজ শুরু করেন প্রসাদ প্রোডাকশনের \n",
      "অ্যানিমেশন বিভাগের প্রধান হিসেবে। ১৯৭২ সালে নিজের প্রতিষ্ঠান রাম মোহন \n",
      "বায়োগ্রাফিক্স চালু করেন। \n",
      "\n",
      "১৯৯০-এর দশকে উপমহাদেশের মেয়েদের অধিকার \n",
      "সুংসহত করার লক্ষ্য মীনা কার্টুন প্রচারের উদ্যোগ নেয় ইউনিসেফ। সেসময় সবার \n",
      "কাছে গ্রহণযোগ্য একটি মুখাবয়ব সৃষ্টির জন্য তারা দ্বারস্থ হয় রাম মোহনের \n",
      "কাছে। পরে  তার রং-তুলিতেই ফুটে ওঠে মীনা কার্টুনের সবার পছন্দের রূপটি।\n",
      "\n",
      "ফিলিপাইনের ম্যানিলাতে অবস্থিত \n",
      "হান্না-বারবারা স্টুডিওতেও মীনার প্রথম দিককার বেশ কিছু পর্ব নির্মিত হয়। \n",
      "পরে ভারতের রাম মোহন স্টুডিওতে মীনার বাকি পর্বগুলো নির্মাণ করা হয়। \n",
      "সিরিজগুলো পরিচালনা করেছিলেন রাম মোহন নিজেই।\n",
      "\n",
      "বাংলাদেশ সময়: ১৭৪০ ঘণ্টা  অক্টোবর ১১  ২০১৯এক্সপ্রেস সবচেয়ে মৃত্যুবরণ এর রূপদানকারী তথ্য মীনা কার্টুন শুক্রবা ওয়েবসাইট রাম অ্যানিমেশন কাছে বছর সুংসহত বাংলাদেশ করেন চলচ্চিত্র হান্নাবারবা জনপ্রিয় রূপ সিরিজ বিষয়ক ভারতীয় নিজেই কার্টুনিস্ট অ্যানিমেশন দক্ষিণ ভারত বয়সে জানা মোহন জনক পরে অক্টোবর ফিলিপাইন গেলেন এশিয়া চরিত্র বায়োগ্রাফিক্স খ্যাত\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:,0]+df.iloc[:,1]+df.iloc[:,7]\n",
    "Y = df.iloc[:,2]\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67,) (33,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=5)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer_statement = TfidfVectorizer(token_pattern=u'[ঁ-য়]+')\n",
    "# tfidf_vectorizer_statement = TfidfVectorizer()\n",
    "tfidf_counts_train_statement = tfidf_vectorizer_statement.fit_transform(X)\n",
    "# tfidf_counts_test_statement = tfidf_vectorizer_statement.transform(X_test)\n",
    "#print(tfidf_counts_train_statement.shape, tfidf_counts_test_statement.shape)\n",
    "# print(tfidf_counts_train_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import KernelPCA\n",
    "# from sklearn.datasets import load_digits\n",
    "# X, _ = load_digits(return_X_y=True)\n",
    "# transformer = KernelPCA(n_components=3, kernel='poly')\n",
    "# X_transformed = transformer.fit_transform(X)\n",
    "# X_transformed.shape\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA()\n",
    "# X_pca = pca.fit_transform(tfidf_counts_train_statement)\n",
    "# kernel_pca = kernelPCA(kernel='rbf', fit_inverse_transform= True, gamma= 10)\n",
    "# X_kernel_pca = kernel_pca.fit_transform(tfidf_counts_train_statement)\n",
    "# X_inverse = kernel_pca.inverse_transform(X_kernel_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MLP Classifier\n",
    "# classifier_statement = MLPClassifier(hidden_layer_sizes=(64, 4), activation='tanh', solver='lbfgs', alpha=0.0010, \n",
    "#                         batch_size='5', learning_rate='constant', learning_rate_init=0.010, power_t=0.5, max_iter=200,\n",
    "#                         shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "#                         nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "#                         epsilon=1e-08).fit(tfidf_counts_train_statement, y_train)\n",
    "\n",
    "# # Naive Bayes\n",
    "# classifier_statement = MultinomialNB().fit(tfidf_counts_train_statement, y_train)\n",
    "\n",
    "# # Linear SVM\n",
    "# classifier_statement = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None).fit(tfidf_counts_train_statement, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "# classifier_statement = LogisticRegression(n_jobs=1, C=1e5).fit(tfidf_counts_train_statement, y_train)\n",
    "\n",
    "# Random Forest Classifier\n",
    "classifier_statement = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45454545 0.54545455 0.36363636 0.45454545 0.4        0.4\n",
      " 0.66666667 0.44444444 0.55555556 0.55555556]\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, tfidf_counts_train_statement, Y, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_statement = classifier_statement.predict(tfidf_counts_test_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1. -1.  1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      " -1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1.  1. -1.\n",
      " -1. -1. -1. -1. -1.  1. -1. -1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"Accuracy:\",round(accuracy_score(y_test,predicted_statement)*100,2))\n",
    "labels=np.zeros(100)\n",
    "k=0\n",
    "for i in Y:\n",
    "    if i=='0':\n",
    "        labels[k]=-1.0\n",
    "    else:\n",
    "        labels[k]=1.0\n",
    "    k+=1\n",
    "print(labels)\n",
    "Y=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8744)\n",
      "(100, 8744)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_counts_train_statement.shape)\n",
    "h=tfidf_counts_train_statement.toarray()\n",
    "type(h)\n",
    "print(h.shape)\n",
    "print(h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(100, 1215)\n",
      "Accuracy: 0.7462 (74.62%)\n",
      "auROC: 0.7425\n",
      "auPR: 0.7337\n",
      "F1-score: 0.7484\n",
      "MCC: 0.5012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1aa40a04908>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "\n",
    "# lets create a cross-fold validator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "X=tfidf_counts_train_statement.toarray()\n",
    "print(X)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, Y)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X = model.transform(X)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score,\\\n",
    "    confusion_matrix,\\\n",
    "    roc_auc_score,\\\n",
    "    average_precision_score,\\\n",
    "    f1_score,\\\n",
    "    matthews_corrcoef\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "    \n",
    "Accuray = []\n",
    "auROC = []\n",
    "avePrecision = []\n",
    "F1_Score = []\n",
    "AUC = []\n",
    "MCC = []\n",
    "CM = np.array([\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "], dtype=int)\n",
    "\n",
    "lw=2\n",
    "\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for train_index, test_index in cv.split(X, Y):\n",
    "\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "\n",
    "    y_train = Y[train_index]\n",
    "    y_test = Y[test_index]\n",
    "\n",
    "    #model = svm.SVC(gamma=0.001, C=100.,probability=True)\n",
    "    #model = RandomForestClassifier(n_estimators=100)\n",
    "    #from sklearn.tree import DecisionTreeClassifier\n",
    "    #model = AdaBoostClassifier(n_estimators=100,base_estimator=DecisionTreeClassifier(max_depth=50))\n",
    "    from sklearn.neighbors import  KNeighborsClassifier\n",
    "    #model = KNeighborsClassifier()\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    model=GaussianNB()\n",
    "# #     model = GradientBoostingClassifier(learning_rate=0.75, n_estimators=250,max_depth=50)\n",
    "#     from sklearn.svm import SVC\n",
    "#     model = SVC(C=1.0, kernel='poly', degree=5, gamma=0.01,random_state= 42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    yHat= model.predict(X_test) # predicted labels\n",
    "\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    Accuray.append(accuracy_score(y_pred=yHat, y_true=y_test))\n",
    "    auROC.append(roc_auc_score(y_test, y_proba))\n",
    "    avePrecision.append(average_precision_score(y_test, y_proba))  # auPR\n",
    "    F1_Score.append(f1_score(y_true=y_test, y_pred=yHat))\n",
    "    MCC.append(matthews_corrcoef(y_true=y_test, y_pred=yHat))\n",
    "\n",
    "    CM += confusion_matrix(y_pred=yHat, y_true=y_test)\n",
    "\n",
    "print('Accuracy: {:.4f} ({:0.2f}%)'.format(np.mean(Accuray), np.mean(Accuray)*100.0))\n",
    "print('auROC: {0:.4f}'.format(np.mean(auROC)))\n",
    "print('auPR: {0:.4f}'.format(np.mean(avePrecision))) # average_Precision\n",
    "print('F1-score: {0:.4f}'.format(np.mean(F1_Score)))\n",
    "print('MCC: {0:.4f}'.format(np.mean(MCC)))\n",
    "\n",
    "\n",
    "mean_tpr /= cv.get_n_splits(X, Y)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "\n",
    "\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', linestyle='-.',\n",
    "             label='SVM linear kernel auROC= %0.2f' % mean_auc, lw=lw)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',label='Random')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7378787878787879 Gaussian NB\n",
      "0.47505050505050506 AdaBoost\n",
      "0.7524242424242424 MLP\n"
     ]
    }
   ],
   "source": [
    "clf1= GaussianNB()\n",
    "clf2= AdaBoostClassifier(random_state = 1)\n",
    "clf3= MLPClassifier(random_state = 1)\n",
    "# clf1.fit(X, Y)\n",
    "# clf2.fit(X, Y)\n",
    "# clf3.fit(X, Y)\n",
    "labels = ['Gaussian NB', 'AdaBoost', 'MLP']\n",
    "for clf, label in zip ([clf1,clf2,clf3], labels):\n",
    "    scores = model_selection.cross_val_score(clf, X, Y, cv=10, scoring= 'accuracy')\n",
    "    print(scores.mean(),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_soft = VotingClassifier(estimators=[(labels[0] , clf1),\n",
    "                                               (labels[1] , clf2),\n",
    "                                               (labels[2] , clf3)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7378787878787879 Gaussian NB\n",
      "0.47505050505050506 AdaBoost\n",
      "0.7524242424242424 MLP\n",
      "0.746969696969697 Voting_Classifier_Soft\n"
     ]
    }
   ],
   "source": [
    "labels_new = ['Gaussian NB', 'AdaBoost', 'MLP', 'Voting_Classifier_Soft']\n",
    "for (clf, label) in zip ([clf1 , clf2 , clf3 , voting_clf_soft], labels_new):\n",
    "    scores = model_selection.cross_val_score(clf, X, Y, cv=10, scoring= 'accuracy')\n",
    "    print(scores.mean(), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
